\section{Literature Review}\label{sec:lit_rev}

\begin{itemize}
    \item Introduction to agent-based modelling
    \item Review of methods for real-time model updating
    \begin{itemize}
        \item Data assimilation
        \item Others
    \end{itemize}
\end{itemize}

As touched upon in Chapter \ref{sec:intro}, the process of developing an
agent-based model typically involves some form of model calibration.
Model calibration is the procedure of fine-tuning the model that we are using
such that it best fits the particular situation that we are seeking to model
\citep{crooks2012introduction}.
There are a large number of different manners in which we can calibrate
agent-based models \citep{thiele2014facilitating}.
These approaches typically involve making use of real-world data to estimate the
parameters and initial state of the model; this is, however, undertaken once
prior to running the model.

In some situations, we aim to simulate events in real-time (or close to
real-time).
In such situations, we are often able to observe the evolution of
the real-world system which we seek to model and consequently may wish to use
this information to recalibrate the model.
This would, however, require that we stop the simulation, undertake calibration,
and restart the model.
We therefore seek an approach that allows us to incorporate observations of the
system whilst simulating the system --- data assimilation.

This Chapter will therefore seek to provide a basic overview of data
assimilation, along with some coverage of the attempts that have been made to
implement such techniques to agent-based models that simulate urban systems.

\subsection{Data Assimilation}\label{sub:lit_rev:da}

%\begin{itemize}
    %\item Origins of filtering with Wiener filter, 1950. Only applies for
        %stationary signals.
    %\item Kalman-Bucy 1961
    %\item Stratonovich 1968
    %\item Jazwinski 1970
    %\item 
%\end{itemize}

The process of data assimilation involves making use of observations along with
prior knowledge (which, in our case, is encoded in a model) to produce
increasingly accurate estimates of variables of interest.
Such a process can be achieved through a Bayesian filtering approach
\citep{jazwinski1970mathematics, bar2004estimation}.

Under such a framework, the updating of the model state is undertaken on the
basis of Bayes Rule (for which a derivation is provided in Appendix
\ref{ch:bayes_rule}):
\begin{equation}
    P(A|B) = \frac{P(B|A) P(A)}{P(B)}
\end{equation}
Bayes Rule is made up of four components:
\begin{enumerate}
    \item $P \left( A \right)$: The probability of $A$, known as the
        \textbf{Prior}.
    \item $P \left( A|B \right)$: The probability of $A$ given $B$, known as the
        \textbf{Posterior}.
    \item $P \left( B|A \right)$: The probability of $B$ given $A$, known as the
        \textbf{Likelihood}.
    \item $P \left( B \right)$: The probability of $B$, known as the
        \textbf{Marginal Likelihood}.
\end{enumerate}
When applying this notation to the problem at hand, the components become:
\begin{enumerate}
    \item \textbf{Prior}, $P(\mathbf{x})$: The probability distribution
        representing the prior state of the model.
    \item \textbf{Posterior}, $P(\mathbf{x}|\mathbf{d})$: The probability
        distribution representing the updated state of the model in light of the
        observed data, that is to say the probability of the model state given
        the data.
    \item \textbf{Likelihood}, $P(\mathbf{d}|\mathbf{x})$: The probability
        distribution of the observed data given the model state.
    \item \textbf{Marginal Likelihood}, $P(\mathbf{d})$: The probability
        distribution representing the observed data.
\end{enumerate}
With the above notation, Bayes Rule becomes:
\begin{equation}
    P \left( \mathbf{x} | \mathbf{d} \right) =
       \frac{P \left( \mathbf{d} | \mathbf{x} \right)
             P \left( \mathbf{x} \right)}{P \left( \mathbf{d} \right)} 
\end{equation}
The aim of a data assimilation scheme therefore becomes to provide an update to
the  state in the form of the posterior, $P \left( \mathbf{x} | \mathbf{d}
\right)$, given new observations, $P \left( \mathbf{d} \right)$.

There exist a number of different schemes for tackling this problem which are
often divided into two groups \citep{talagrand1997assimilation}:
\begin{enumerate}
    \item \textbf{Sequential}: Upon the arrival of a new observation, the model
        state is updated at the time of the new observation; includes Kalman
        Filter (and variations thereof), Particle Filter.
    \item \textbf{Variational}: Upon the arrival of a new observation, the model
        solutions are updated at all times simultaneously; includes 3D-VAR,
        4D-VAR.
\end{enumerate}

Of the work that currently exists wherein investigators attempt to apply data
assimilation schemes to agent-based models, most make use of sequential schemes. 

\subsection[Application of Data Assimilation to Agent-Based Models]{Application of Sequential Data Assimilation to Agent-Based
Models}\label{sub:lit_rev:da_abm}

%Agent-based simulation is useful for studying people's movement in smart
%environment.
%Existing agent-based simulations are typically used as offline tools that help
%system design.
%They are not dynamically data-driven because they do not utilise any real time
%sensor data of the environment.
%In this paper, we present a method that assimilates real time sensor data into
%an agent-based simulation model.
%The goal of data assimilation is to provide inference of people's occupancy
%information in the smart environment, and thus lead to more accurate simulation
%results.
%We use particle filters to carry out the data assimilation and present some
%experiment results, and discuss how to extend this work for more advanced data
%assimilation in agent-based simulation of smart environment.

%\subsection{Rai 2013}

%Agent-based simulation is useful for studying human activity and their
%interactions in smart environments.
%Existing agent-based simulations are mostly offline tools that do not utilise
%real time information of smart environments.
%In previous work we developed a particle filter-based data assimilation method
%to assimilate real time sensor data from the environment into an agent-based
%simulation model.
%This paper extends the previous work by presenting a framework of behaviour
%pattern informed data assimilation.
%We describe the structure of this framework and focus on the task of behaviour
%pattern detection using Hidden Markov Model.
%We apply behaviour pattern detection to a smart office case study example and
%discuss how the detected behaviour pattern can inform the data assimilation in
%agent-based simulation of smart environments.

%\subsection{Wang 2015}

%Agent-based simulations are useful for studying people's movement and to help
%making decisions in situations like emergency evacuation in smart environments.
%These agent-based simulations are typically used as offline tools and do not
%assimilate real time data from the environment.
%With more and more smart buildings equipped with sensor devices, it is possible
%to utilise real time sensor data t dynamically inform the simulations to
%improve simulation results.
%In this paper, we propose a method to assimilate real time sensor data in
%agent-based simulations of smart environments based on particle filters.
%The data assimilations aims to estimate the system state, i.e. people's location
%information in real time, and use the estimated states to provide initial
%conditions for more accurate simulation/prediction of the system dynamics in the
%future.
%We develop a particle filter-based data assimilation framework and propose a new
%resampling method named as component set resampling to improve data assimilatoin
%for multiple agents.
%The proposed framework and method are demonstrated and evaluated through
%experiments using a sparsely populated smart environment.

%\subsection{Ward 2016}

%A widespread approach to investigating the dynamical behaviour of complex socia
%systems is via agent-based models.
%In this paper, we describe how such models can be dynamically calibrated using
%the ensemble kalman filter, a standard method of data assimilation.
%Our goal is twofold.
%First, we want to present the enkf in a simple setting for the benefit of abm
%practitioners who are unfamiliar with it.
%Second, we want to illustrate to data assimilation experts the value of using
%such methods in the context of abms of complex social systems and the new
%challenges these types of model present.
%We work towards these goals within the context of a simple question of practical
%value: how many people are there in Leeds (or any other major city) right now?
%We build a hierarchy of exemplar models that we use to demonstrate how to apply
%the enkf and calibrate these using open data of footfall counts in Leeds.

%\begin{itemize}
    %\item Environment:
    %\item Data source: synthetic data generated by the model.
    %\item Type of data: Sensors report presence of agent at position.
    %\item Method: Particle filter
    %\item Experiments run: different routing types, different number of
        %particles, single agent, two agents (set up such that agents are less
        %likely to interact).
    %\item Findings:
%\end{itemize}

\subsubsection{Particle Filter-based Approaches}\label{subs:lit_rev:da_abm:pf}

One of earliest pieces of work undertaken on the application of data
assimilation schemes to agent-based models of urban environments was by
\citet{wang2013data}.
In this work, they simulate a smart office environment with people in it --- a
scenario that is becoming increasingly common with the advent of the Internet of
Things \citep{zanella2014internet}.
The aim of the work was to make use of real-time data in conjunction with the
agent-based simulation to provide more accurate estimates of the occupancy of
the environment.
This was achieved using the Particle Filter method of data assimilation; the
method was chosen as it did not require the system to be Gaussian.
The particle filter method operates by holding an ensemble of realisations of
the simulation, each of which are evolved forward over time between
observations; when observations are received, the particle states are weighted
and the new state is obtained by sampling from these weighted particles.
The observations used were synthetic data generated by the agent-based model,
aiming to emulate motion sensors which would provide a binary response of
whether a person was present in a given location.

The work undertaken consisted of two experiments --- firstly simulating single
agent in the environment, then going on to simulate two agents in the same
environment.
In the case if the first experiment, the agent was simulated with two different
routing behaviours; for the first routing behaviour, the agent move forward
sequentially through a series of waypoints, whilst for the second routing
behaviour, the agent moves through a series of waypoints before turning back to
return to its initial position.
In this experiment, it was found that the simulation error decreased when the
agent was detected at each of the sensors for both routing behaviours with error
growing between detections; this is as expected --- it confirms that the
simulation becomes more accurate with the addition of further information
regarding the system.
In the case of the second routing behaviour, the simulation error also grew
following the agent's turn to head back to its origin.
In the second experiment, they aimed to simulate two agents in the same
environment, with the two agents maintaining spatial separation.
This simulation was run a number of times for different numbers of particles
with a view to establishing a relationship between the number of particles and
simulation accuracy.
It was found that as the number of particles was increased (through $400$,
$800$, $1200$ and $1600$), the simulation error decreased.
It was also found, however, that the experiments with fewer particles ($400$ and
$800$) struggled to converge, with the smaller number of particles unable to
provide sufficient coverage of the state space.
It was therefore noted that as the number of agents was increased, the method
was likely to struggle.

This final issue may be solved by an increase in the number of particles;
however, this comes with an attached increase  in the computational cost (both
in terms of compute time and space).
The implementation of the particle filter requires that a realisation of the
model be kept for each particle, resulting in growing memory requirements as
the number of particles are increased.
Furthermore, each particle is required to evolve the model for each time-step,
resulting in an increasing computational cost.

There are two subsequent pieces of research which have sought to build directly
upon the above initial investigation; each of them make use of the same
simulation model, adding different developments.

The first of these was undertaken by \citet{rai2013behavior} and aimed to add a
further layer by estimating behavioural patterns in the system.
This was achieved using a hidden Markov model which was trained on the
historical data of the system.
The information encoded in the hidden Markov model is incorporated into the
particle filter at the sampling step; the hidden Markov model is first used to
identify the types of behaviour being exhibited by the system and particles then
sample from different types, subsequently engaging the simulation with said
behaviour.
The remaining steps of the filtering process remained the same.
The aim of applying this approach was to further improve the accuracy of
simulations through the identification of pedestrian behaviours.
In order to do so, the behaviours were divided into the following categories:
\begin{itemize}
    \item \textbf{Outside}: All agents wait outside of a conference room.
    \item \textbf{In Conference}: All agents are attending meeting in the
        conference room.
    \item \textbf{Few Entering}: A small number of agents entering the
        conference room.
    \item \textbf{High Entering}: A large number of agents entering the
        conference room.
    \item \textbf{Few Leaving}: A small number of agent leaving the conference
        room.
    \item \textbf{High Leaving}: A large number of agent leaving the conference
        room.
\end{itemize}
These categorisations, particularly the \textit{entering} and \textit{leaving}
states, present a problem.
When entering the conference room, agents are more likely to be categorised
into three groups --- early few entering, high entering and late few entering
--- this re-categorisation helps to describe the status of the agents that are
not in the process of entering:
\begin{itemize}
    \item \textbf{Early few entering}: A small number of agents entering the
        conference room early before a meeting, with the other agents outside
        the conference room.
    \item \textbf{High entering}: A large number of agents entering the
        conference room.
    \item \textbf{Late few entering}: A small number of agents entering the
        conference room late after the meeting has started, with the other
        agents inside the conference room.
\end{itemize}
The investigation then attempts to determine the accuracy with which the model
is able to identify the correct behaviour for agents, with accuracy being
defined as
\begin{equation}
    \frac{1}{T} \sum_{k=1}^T S_{t}^{k} - S_{t}^{real}
\end{equation}
where $T$ is taken to be the total number of simulation steps and $S$ is the
behaviour pattern state.
It is unclear, however, how this calculation is undertaken given that the
behavioural states are categories and not numerical; furthermore, the meaning
behind the state notation is explained --- it appears that $k$ is a time-step
index, however it does not make sense to compare the behavioural state at each
time state to a static ``real'' behavioural state and the latter would likely be
a transient property.
Some indication is given that a numerical encoding of the categories has been
used for visualisation purposes, however this should not be used for arithmetic
purposes, nor should any ordinality be inferred from it.
Indeed, the results presented take the form of accuracy percentages, suggesting
that a more conventional accuracy score has been used, such as
\begin{equation}
    \frac{1}{T} \sum_{k=1}^{T} \mathds{1} \left(
                \hat{S}_k = S_{k}^{real} \right)
\end{equation}
where the indicator function, $\mathds{1} \left( \ldots \right)$, returns $1$
when the condition is fulfilled, else $0$.
The results suggest that the model developed accurately identifies the
behavioural states except for states that occur infrequently; the model performs
particularly well when identifying states in which agents are static, i.e.
\textit{outside} and \textit{in conference}.
Such supplementary information could improve the performance of data driven
simulations, likely helping the process of data assimilation for parameter
estimation.
It is worth considering, however, that the addition of this further layer to the
assimilation process would also result in a further increase in the
computational cost.

The second of the investigations to develop on the initial work by
\citet{wang2013data} was undertaken by \citet{wang2015data}, this time making
use of three different resampling schemes: standard resampling (as seen in the
original investigation), component set resampling and mixed component set
resampling.
Given these resampling schemes, they undertake four experiments.

The first of these experiments seeks to address the use of component set
resampling.
This is achieved by testing the implementation for increasing numbers of agent
with component set resampling, and comparing against the corresponding results
when using standard resampling.
It was found that, when using standard resampling, the number of particles in
the ensemble that matched with observations decreased as the number of agents
increased; the use of mixed component resampling was found to reduce the rate at
which this occurred.

The second of the experiments aims to assess the effectiveness of the particle
filter using the standard resampling scheme when simulating one agent.
For this experiment, the agent was imparted with two different behaviours as in
the original investigation.
The effectiveness of the data assimilation scheme was assess by measuring the
average distance per particle between the simulated agent and the real agent.
It was once again found that the simulation error fell with each observation.
Furthermore, it was noted that there were two situations that caused an increase
in the simulation error:
\begin{enumerate}
    \item The agent turning back on itself in an area where no sensors are
        present.
    \item The agent approaches a 3-way intersection, where the agent is offered
        discretely different option of direction in which to travel; the
        particles, therefore, struggle to converge on the true state.
\end{enumerate}

The third experiment aims to asses the effectiveness of standard resampling when
applying the scheme to a system containing two agents.
This was achieved by comparing the average error per agent per particle for
different numbers of particles (1200 particles, 1600 particles and 2000
particles).
It was found that as the number of particles increased, the simulation error
decreased.

The final experiment aimed to similarly assess the effectiveness mixed component set
resampling for a system containing multiple agents, first starting with two
agents.
It was found that for each of the options for number of particles, the
implementation of mixed component set resampling reduced the simulation error.
Furthermore, when considering systems containing more agents (four or six), it
was also found that the implementation of mixed component set resampling
improved the simulation accuracy; however, as the number of agents in the system
increased, this improvement reduced.
This was attributed to situations when agents would crowd together, thus causing
difficulties for the particles to distinguish different agents from binary
sensors.

\subsubsection{Kalman Filter-based Approaches}\label{subs:lit_rev:da_abm:kf}

Other investigations have sought to apply different data assimilation schemes
including the Ensemble Kalman Filter.
As shall be explained in Section \ref{ch:method}, the Ensemble Kalman Filter is an
adaptation of the original Kalman Filter \citep{evensen2003ensemble}.
This data assimilation technique was implemented in the investigation by
\citet{ward2016dynamic}, which sought to expose agent based modelling
practitioners to the technique in the context of modelling how many people are
in a major city at a given time.
This investigation consisted of two experiments.
In the first experiment, the Ensemble Kalman Filter was implemented with a
simple box model that estimated how many people were present in the box based on
probabilities of people entering and exiting.
In the second experiment, the Ensemble Kalman Filter was applied to an
epidemic-like model in which the population was split into workers and shoppers,
with works either being at home or at work, and shoppers either being
susceptible to going shopping, shopping in town or having returned home after
shopping.

The first experiment made use of synthetic data generated using the model with
randomly drawn parameter values in order to produce ground truth data.
Observations were then generated by adding normally distributed random noise to the
ground truth.
Running the filtering process with an ensemble of $100$ realisations, data
assimilation for state estimation was performed at each time-step, and it was
found that on average, the error (with respect to the synthetic ground truth) of
the model state was smaller after assimilation, as well as being smaller than
the error in the observations.
Furthermore, this approach also outperforms the theoretical steady state
calculated for the system.

Beyond this, the first experiment also aimed to carry out parameter estimation
by including the parameters, which are assumed to be unknown, in the state vector.
In doing so, estimates of the parameters are produced at both the forecasting
stage and the updating stage.
The filtering process succeeds in reducing the error in the parameters with
respect to the ground truth; despite this, the parameter estimates do not
converge on their true values, underestimating both the arrival rate and
departure rate.
The ratio of the two parameters, however, is correctly estimated, suggesting
that the data assimilation process has correctly estimated the governing
dynamics.

The second experiment aimed to model pedestrians arriving and departing at
Briggate in Leeds.
Pedestrians are divided into shoppers and workers, with each group being
governed by epidemic-like dynamics.
Shoppers are either at home before shopping (susceptible), in town shopping
(infected) or at home having returned from shopping (recovered); workers are
either at home or at work.
This approach seeks to more realistically represent pedestrian behaviour,
designating different types of people and introducing more complex behaviours
for agents deciding to enter the city.
The data used was sourced from a footfall camera on Briggate which recorded
hourly counts of the number of pedestrians arriving; as such, the primary target
of the assimilation process was the cumulative count of the number of agents to
have arrived in the city, combining both shoppers and workers.
The Ensemble Kalman Filter was applied using different ensemble sizes ($10$,
$100$ and $1000$), and it was found that as the ensemble size increased, the
accuracy of the simulation improved; it was noted, furthermore that the
improvement observed between ensemble sizes of $10$ and $100$ were much greater
than between ensemble sizes of $100$ and $1000$.
Once again, parameter estimation was undertaken; however, in this case a
particle filter-like approach was used.

Whilst this investigation displays that the Ensemble Kalman Filter can be
implemented in conjunction with an agent-based model to successfully improve the
model's prediction accuracy, it suffers from a number of shortcomings.
First and foremost, it should be noted that the models used for the
investigation are very simple in comparison to the majority of agent-based
models; indeed, the authors admit that the models used for the investigation
were not developed in the standard object-oriented framework typically used in
agent-based modelling.
The model used in the first experiment is a binary state model, with each agent
either being in the city or not in the city.
The inter-agent interaction governing their transition between the two states is
global --- this is to say that each agent's decisions are based on the state of
every other agent without considering more intricate mechanisms of attraction
and repulsion between agents \citep{helbing1995social} such as spatially local
ones.
Whilst the second experiment seeks to include a richer set of behaviours by
further segmenting the agents, it still fails to include any spatial aspect,
with agents again able to interact in a homogeneous fashion.
In the case of each of the experiments, data assimilation is used to perform
both state estimation and parameter estimation.

Beyond this, it should be noted that the inclusion of parameter estimation,
whilst not uncommon, is not a standard approach and so some attention should be
given to the impact of its inclusion in the procedure.

%\subsection{Other Approaches}\label{sub:lit_rev:da_abm:other}



%Maritime piracy is posing a genuine threat to maritime transport.
%The main purpose of simulation is to predict behaviours of many actual systems,
%and it has been successfully applied in many fields.
%But the application of simulation in the maritime domain is still scarce.
%The rapid development of network and measurement technologies brings about
%higher accuracy and better availability of online measurements.
%This makes the simulation paradigm names as dynamic data driven simulation
%increasingly popular.
%It can assimilation the online measurements into the running simulation models
%and ensure much more accurate prediction of the complex systems under study.
%In this paper, we study how to utilise the online measurements in the agent
%based simulation of the maritime pirate activity.
%A new random finite set based data assimilation algorithm is proposed to
%overcome the limitations of the conventional vectors based data assimilation
%algorithms.
%The random finite set based general data model, measurement model, and
%simulation model are introduced to support the proposed algorithm.
%The details of the proposed algorithm are presented in the context of agent
%based simulation of maritime pirate activity.
%Two groups of experiments are used to practically prove the effectiveness and
%superiority of the proposed algorithm.

%\begin{itemize}
    %\item \cite{ward2016dynamic} --- model of pedestrians on Briggate, a 1-D
        %strip along which pedestrians walk, using enkf for both state and
        %parameter calibration.
    %\item \cite{wang2013data, wang2015data} --- agents occupying a smart
        %environment/building with a view to modelling population density, using
        %particle filter.
    %\item \cite{rai2013behavior} --- agents occupying a smart office/building,
        %using particle filter, extends \cite{wang2013data, wang2015data} by
        %incorporating a Hidden Markov Model for behaviour pattern detection.
    %\item \cite{wang2017random} --- model of maritime pirates, using random
        %finite set based data assimilation instead of kf or pf --- why? does
        %this have any relevance?
%\end{itemize}

%\subsection{Data Assimilation with Cellular Automata}\label{sub:lit_rev:da:ca}


%\begin{itemize}
    %\item WHAT ARE CELLULAR AUTOMATA
    %\item HOW DO THEY RELATE TO ABMS?
    %\item WHAT IS THE POINT OF INCLUDING THIS?
%\end{itemize}

%\begin{itemize}
    %\item \cite{li2017exploring} --- CA for urban land use, using enkf.
    %\item \cite{li2012assimilating} --- CA for urban land use, using enkf.
    %\item 
%\end{itemize}

\subsection{Summary}\label{sub:lit_rev:summary}

%Drawing all of the information together.

%Issues to consider:
%\begin{itemize}
    %\item What happens when there is incomplete information? i.e. data is not
        %provided as frequently as we would like.
    %\item What happens when agent are given discrete choices that alter their
        %potential future states in a non-linear fashion?
    %\item Can better information regarding agent behaviour be used to improve
        %data assimilation?
    %\item EnKF vs PF: typically don't need as big ensemble sizes with EnKF, but
        %might not be able to handle models.
%\end{itemize}

As has been seen, there exist a small number of investigations which attempt to
implement sequential data assimilation schemes in conjunction with agent-based
pedestrian models; in each case, the filtering process lead to improvements in
the modelling accuracy.
Much of the work that has been undertaken has used the Particle Filter, and has
made use of synthetic data; the ultimate goal of developing such a technology
will inevitably be to use it with real-world data which is generated in
real-time.

Each of the two data assimilation schemes that have been used have their own
strengths and weaknesses.
The Ensemble Kalman Filter used by \citet{ward2016dynamic} are typically run
with smaller ensemble sizes, but struggle in cases when probability
distributions are non-normal. 
The Particle Filters used by \citet{wang2013data, rai2013behavior,
wang2015data} resolve this issues, offering an exact solution at the cost of
requiring larger ensemble sizes and consequently greater computation space and
time.

This work therefore seeks to build upon the work by \citet{ward2016dynamic} in
implementing an Ensemble Kalman Filter in conjunction with a agent-based
pedestrian model.

As an additional note, whilst this dissertation focuses on the application of
data assimilation methods to agent-based models, there also exists a body of
work that makes use of the same methods in conjunction with cellular automata
\citep{li2012assimilating, li2017exploring}.
This may be of interest to those concerned more broadly with the development of
real-time social simulation.

%Cellular automata can be thought of as a specific case of agent-based models in
%which the environment is characterised by a discrete grid, with the occupancy of
%each grid cell being updated over time based on the state of its neighbouring
%cells.
