\section{Method}\label{sec:method}

%\begin{itemize}
    %\item Data assimilation
    %\begin{itemize}
        %\item Kalman Filter
        %\item Ensemble Kalman Filter
        %\item Flow diagram for each algorithm
    %\end{itemize}
    %\item Model
    %\begin{itemize}
        %\item Model ODD
        %\item Model flow diagram
        %\item Agent behaviour flow diagram
    %\end{itemize}
    %\item Experiments
    %\begin{itemize}
        %\item Tracking two agents in the environment
        %\item Variation of observation noise
        %\item Variation of assimilation period
        %\item Variation of ensemble size
        %\item Providing aggregated data
    %\end{itemize}
%\end{itemize}

There exist a number of different data assimilation schemes, many of which are
used extensively in fields such as numerical weather prediction, navigation and
tracking, and other forecasting problems.
Such methods, however, have been sparsely used in the field of real-time urban
simulation and as such this investigation attempts to build upon the existing
work by implementing the Ensemble Kalman Filter in conjunction with a pedestrian
agent-based model.
This chapter therefore seeks to outline the method used in this investigation --- the
Ensemble Kalman Filter. 
As shall be explained, the Ensemble Kalman Filter is an approximation of the
Kalman Filter, and as such some attention will first be given to the original
Kalman Filter, followed by an explanation of the Ensemble Kalman Filter along
with the innovations that it incorporates.
Finally, the model used for this investigation will be briefly detailed, along
with the way in which the Ensemble Kalman Filter is applied to the model.

\subsection{Kalman Filter}\label{sub:method:kf}

%One of the earliest forms of Bayesian filtering is known as Wiener filtering
%\citep{wiener1950extrapolation}, which is used in the field of signal
%processing.
One of the earliest forms of Bayesian filtering is the sequential data
asssimilation scheme known as the Kalman Filter, which forms the foundation of
this piece of work.
As with other sequential data assimilation schemes, the Kalman Filter operates
on a model with a given state and forecasting process, updating the state and
associated covariance matrix upon receipt of new observations.
This is undertaken as part of the predict-update cycle.
The prediction process is undertaken by applying the modelling operator to both
the model state and model state covariance.
The update process is undertaken based on the uncertainty in the model forecasts
and the observation uncertainty with a view to minimising the posterior mean
squared error.
Under the Bayesian framework outlined in Section \ref{sub:lit_rev:da}, we refer
to the model state vector and associated covariance before updating as the prior,
$\mathbf{x}$ and $\mathbf{Q}$, and to the model state and associated covariance
after updating as the posterior, $\hat{\mathbf{x}}$ and $\hat{\mathbf{Q}}$.
Given new observations, $\mathbf{d}$, the posterior state vector is given by
\begin{equation}
    \hat{\mathbf{x}} = \mathbf{x} + \mathbf{K} \left(
                        \mathbf{d} - \mathbf{H} \mathbf{x} \right),
\end{equation}
and the posterior covariance is given by
\begin{equation}
    \hat{\mathbf{Q}} = \left( \mathbf{I} - \mathbf{K} \mathbf{H} \right)
                        \mathbf{Q},
\end{equation}
where $\mathbf{K}$ is the Kalman gain matrix and $\mathbf{H}$ is the observation operator.
The Kalman gain matrix is given by
\begin{equation}
    \mathbf{K} = \mathbf{Q} \mathbf{H}^T \left(
                    \mathbf{H} \mathbf{Q} \mathbf{H}^T + \mathbf{R}
                 \right) ^ {-1}
\end{equation}
where $\mathbf{R}$ is the observation covariance.

In the case when the errors are normally distributed, this filter provides an
exact posterior estimate.
However, this approach suffers from two issues.
The first of these is that it assumes that the operators that are applied
(namely the model transition operator and the observation operator) are linear;
this is often not the case with more complex systems, particularly when the
system elements interact with each other (as is typically the case in
agent-based models).
Furthermore, as the dimensionality of the model increases, the cost of
propagating forward the model state covariance may increase to the point where
it is intractable.

A number of approaches have been developed which attempt to solve these
problems, one of which is the Ensemble Kalman Filter.

%As outlined in Section \ref{sec:lit_rev:da}, this can be encoded in Bayes
%Theorem:
%\begin{equation}
    %P \left( \mathbf{x} | \mathbf{d} \right) = 
    %\frac{P ( \mathbf{d} | \mathbf{x} ) P ( \mathbf{x} )}{P ( \mathbf{d} ) }.
%\end{equation}

%\begin{itemize}
    %\item When is it bad?
    %\item What can we do to improve it?
    %\item The KF provides us with the exact posterior estimate when errors are
        %normally distributed.
    %\item KF is based on linear dynamical system --- that is to say that both of
        %the matrices $\mathbf{M}$ (the model forecast operator) and $\mathbf{H}$
        %(the observation operator) perform linear transformations.
    %\item This is often not the case with more complex systems, particularly
        %when the system elements interact with each other (as is typically the
        %case in Agent-Based Models).
    %\item 
%\end{itemize}


\subsection{Ensemble Kalman Filter}\label{sub:method:enkf}

%Problems with the Kalman Filter:
%\begin{itemize}
    %\item it assumes Gaussian PDFs
    %\item it assumes linear model
    %\item Cost of evolving covariance matrix
%\end{itemize}

In order to address some of these problems, the Ensemble Kalman Filter was
developed \citep{evensen2003ensemble, evensen2009ensemble}, which acts as an
approximation of the Kalman Filter.
This approximation is achieved through a Monte Carlo approach of using an
ensemble of sample state vectors to represent the state distribution; this
development mirrors the recent incorporation of Monte Carlo methods in the field
of Bayesian statistics \citep{wikle2007bayesian}.
The remainder of this section will seek to outline this framework in a manner
similar to that documented by \citet{mandel2009brief}.
The state is represented as an ensemble of state vectors as follows:
\begin{equation}
    \mathbf{X} = \left[ \mathbf{x}_1, \ldots, \mathbf{x}_N \right]
               = \left[ \mathbf{x}_i \right], \quad \forall i \in (1, N),
\end{equation}
where the state ensemble matrix, $\mathbf{X}$, consists of $N$ state vectors,
$\mathbf{x}_i$.
The mean state vector, $\bar{\mathbf{x}}$, can be found by averaging over the
ensemble:
\begin{equation}
    \bar{\mathbf{x}} = \sum_{i = 1}^{N} \mathbf{x}_i.
\end{equation}
Similarly, the observations are represented as follows:
\begin{equation}
    \mathbf{D} = \left[ \mathbf{d}_1, \ldots, \mathbf{d}_N \right]
               = \left[ \mathbf{d}_i \right], \quad \forall i \in (1, N),
\end{equation}
with each member of the data ensemble matrix, $\mathbf{D}$, being the sum of the
original observation $\mathbf{d}$, and a random vector, $\mathbf{\epsilon}_i$:
\begin{equation}
    \mathbf{d}_i = \mathbf{d} + \mathbf{\epsilon}_i, \quad
                   \forall i \in (1, N).
\end{equation}
The random vector is drawn from an unbiased normal distribution:
\begin{equation*}
    \mathbf{\epsilon} \sim \mathcal{N} (0, \mathbf{R}).
\end{equation*}
As with the model state, the mean data vector, $\bar{\mathbf{d}}$, can be found
by averaging over the ensemble:
\begin{equation}
    \bar{\mathbf{d}} = \sum_{i = 1}^{N} \mathbf{d}_i.
\end{equation}
Given that the noise added to the original data vector is unbiased, we should
expect that the mean data vector converges to the original data vector,
$\mathbf{d}$:
\begin{equation*}
    \lim_{N \to \infty} \bar{\mathbf{d}} = \mathbf{d}.
\end{equation*}

By implementing these adaptations, the Ensemble Kalman Filter aims to address
the issues faced by the original Kalman Filter with respect to forecasting the
state covariance matrix; more specifically, as a result of this approach the
state covariance matrix no longer needs to be forecast by applying the model
operator, but instead can simply be generated as a sampling covariance.
Consequently, concerns over the computational cost of forecasting the covariance
matrix and over the requirement that the forecasting process be undertaken by
applying a linear operator to the covariance matrix are greatly reduced.

Given the above framework, the data assimilation is once again made up of the
predict-update cycle, with the updating of the state ensemble,
$\hat{\mathbf{X}}$, begin undertaken on the basis of the following equation: 
\begin{equation}
    \hat{\mathbf{X}} = \mathbf{X} + \mathbf{K}
                        \left(
                        \mathbf{D} - \mathbf{H} \mathbf{X}
                        \right),
\end{equation}
where $\mathbf{H}$ is once again the observation operator. 
In this case, the Kalman gain matrix, $\mathbf{K}$ is given by
\begin{equation}
    \mathbf{K} = \mathbf{C} \mathbf{H}^T
                     {\left(
                     \mathbf{H} \mathbf{C} \mathbf{H}^T
                     + \mathbf{R}
                     \right)} ^ {-1}.
\end{equation}
in which the previous state covariance matrix, $\mathbf{Q}$, has been replace
with the sample state covariance matrix, $\mathbf{C}$.

